{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714c2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_37604\\1704730133.py:23: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "After loading shape: (1000098, 56)\n",
      "Handling missing values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\modeling.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n",
      "c:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\modeling.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first impute shape: (1000098, 56)\n",
      "Encoding categorical variables...\n",
      "After encoding shape: (1000098, 1878)\n",
      "Handling missing values after encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\modeling.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second impute shape: (1000098, 1878)\n",
      "Preparing data for Claim Severity Prediction...\n",
      "After filtering TotalClaims > 0 shape: (2788, 1878)\n",
      "X_train_severity shape: (2230, 1877)\n",
      "y_train_severity shape: (2230,)\n",
      "Checking for missing values in training features and target...\n",
      "NaNs in X_train_severity: 4460\n",
      "NaNs in y_train_severity: 0\n",
      "Dropping rows with NaNs in training data...\n",
      "After dropping, NaNs in X_train_severity: 0\n",
      "After dropping, NaNs in y_train_severity: 0\n",
      "Training regression models...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1877)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Train models for claim severity prediction\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining regression models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m regression_models = \u001b[43mtrain_regression_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_severity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_severity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Evaluate regression models\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating regression models...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\modeling.py:74\u001b[39m, in \u001b[36mtrain_regression_models\u001b[39m\u001b[34m(X_train, y_train)\u001b[39m\n\u001b[32m     68\u001b[39m models = {\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLinearRegression\u001b[39m\u001b[33m\"\u001b[39m: LinearRegression(),\n\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m: RandomForestRegressor(random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m: XGBRegressor(random_state=\u001b[32m42\u001b[39m, verbosity=\u001b[32m0\u001b[39m)\n\u001b[32m     72\u001b[39m }\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\venv\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:618\u001b[39m, in \u001b[36mLinearRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    614\u001b[39m n_jobs_ = \u001b[38;5;28mself\u001b[39m.n_jobs\n\u001b[32m    616\u001b[39m accept_sparse = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.positive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcoo\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m has_sw = sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\matos\\tenx 10academy\\week 3\\End-to-End Insurance Risk Analytics & Predictive Modeling\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1128\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1126\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1131\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1132\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1135\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 1877)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "# Task 4: Predictive Modeling Notebook\n",
    "\n",
    "# **1. Import Necessary Libraries and Functions**\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src/utils'))\n",
    "from modeling import (\n",
    "    handle_missing_data,\n",
    "    encode_categorical_variables,\n",
    "    split_data,\n",
    "    train_regression_models,\n",
    "    evaluate_regression_models,\n",
    "    train_classification_models,\n",
    "    evaluate_classification_models,\n",
    "    analyze_feature_importance\n",
    ")\n",
    "\n",
    "# **2. Load and Preprocess the Dataset**\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "data_path = \"../data/processed/claim_metrics.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print shape after each major step for debugging\n",
    "def print_shape(label, df):\n",
    "    print(f\"{label} shape: {df.shape}\")\n",
    "\n",
    "print_shape(\"After loading\", df)\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "df = handle_missing_data(df, method=\"impute\")\n",
    "print_shape(\"After first impute\", df)\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "df = encode_categorical_variables(df)\n",
    "print_shape(\"After encoding\", df)\n",
    "\n",
    "# Impute again after encoding to ensure no NaNs remain\n",
    "print(\"Handling missing values after encoding...\")\n",
    "df = handle_missing_data(df, method=\"impute\")\n",
    "print_shape(\"After second impute\", df)\n",
    "\n",
    "# **3. Data Preparation for Claim Severity Prediction**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Claim Severity Prediction...\")\n",
    "target_col_severity = \"TotalClaims\"\n",
    "severity_data = df[df[target_col_severity] > 0]  # Use subset where claims > 0\n",
    "print_shape(\"After filtering TotalClaims > 0\", severity_data)\n",
    "X_train_severity, X_test_severity, y_train_severity, y_test_severity = split_data(severity_data, target_col_severity)\n",
    "print_shape(\"X_train_severity\", X_train_severity)\n",
    "print_shape(\"y_train_severity\", y_train_severity)\n",
    "\n",
    "# Check for missing values in features and target before training\n",
    "print(\"Checking for missing values in training features and target...\")\n",
    "print(\"NaNs in X_train_severity:\", X_train_severity.isna().sum().sum())\n",
    "print(\"NaNs in y_train_severity:\", y_train_severity.isna().sum())\n",
    "\n",
    "if X_train_severity.isna().sum().sum() > 0 or y_train_severity.isna().sum() > 0:\n",
    "    print(\"Dropping rows with NaNs in training data...\")\n",
    "    nan_mask = ~(X_train_severity.isna().any(axis=1) | y_train_severity.isna())\n",
    "    X_train_severity = X_train_severity[nan_mask]\n",
    "    y_train_severity = y_train_severity[nan_mask]\n",
    "    print(\"After dropping, NaNs in X_train_severity:\", X_train_severity.isna().sum().sum())\n",
    "    print(\"After dropping, NaNs in y_train_severity:\", y_train_severity.isna().sum())\n",
    "\n",
    "# Train models for claim severity prediction\n",
    "print(\"Training regression models...\")\n",
    "regression_models = train_regression_models(X_train_severity, y_train_severity)\n",
    "\n",
    "# Evaluate regression models\n",
    "print(\"Evaluating regression models...\")\n",
    "severity_results = evaluate_regression_models(regression_models, X_test_severity, y_test_severity)\n",
    "print(\"Severity Prediction Results:\")\n",
    "print(severity_results)\n",
    "\n",
    "# **4. Data Preparation for Premium Optimization**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Premium Optimization...\")\n",
    "target_col_premium = \"CalculatedPremiumPerTerm\"\n",
    "X_train_premium, X_test_premium, y_train_premium, y_test_premium = split_data(df, target_col_premium)\n",
    "\n",
    "# Train models for premium prediction\n",
    "print(\"Training regression models...\")\n",
    "premium_models = train_regression_models(X_train_premium, y_train_premium)\n",
    "\n",
    "# Evaluate premium prediction models\n",
    "print(\"Evaluating regression models...\")\n",
    "premium_results = evaluate_regression_models(premium_models, X_test_premium, y_test_premium)\n",
    "print(\"Premium Prediction Results:\")\n",
    "print(premium_results)\n",
    "\n",
    "# **5. Claim Probability Prediction (Advanced Task)**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Claim Probability Prediction...\")\n",
    "target_col_probability = \"ClaimOccurred\"  # Binary classification (1 if claim occurred, 0 otherwise)\n",
    "X_train_prob, X_test_prob, y_train_prob, y_test_prob = split_data(df, target_col_probability)\n",
    "\n",
    "# Train classification models\n",
    "print(\"Training classification models...\")\n",
    "classification_models = train_classification_models(X_train_prob, y_train_prob)\n",
    "\n",
    "# Evaluate classification models\n",
    "print(\"Evaluating classification models...\")\n",
    "probability_results = evaluate_classification_models(classification_models, X_test_prob, y_test_prob)\n",
    "print(\"Claim Probability Prediction Results:\")\n",
    "print(probability_results)\n",
    "\n",
    "# **6. Feature Importance Analysis**\n",
    "print(\"Analyzing feature importance...\")\n",
    "for model_name, model in regression_models.items():\n",
    "    print(f\"Analyzing feature importance for {model_name}...\")\n",
    "    analyze_feature_importance(model, X_train_severity)\n",
    "\n",
    "# **7. Save Results**\n",
    "print(\"Saving results...\")\n",
    "severity_results_path = \"../results/severity_results.csv\"\n",
    "severity_results_df = pd.DataFrame.from_dict(severity_results, orient=\"index\")\n",
    "severity_results_df.to_csv(severity_results_path)\n",
    "print(f\"Severity results saved to {severity_results_path}.\")\n",
    "\n",
    "premium_results_path = \"../results/premium_results.csv\"\n",
    "premium_results_df = pd.DataFrame.from_dict(premium_results, orient=\"index\")\n",
    "premium_results_df.to_csv(premium_results_path)\n",
    "print(f\"Premium results saved to {premium_results_path}.\")\n",
    "\n",
    "probability_results_path = \"../results/probability_results.csv\"\n",
    "probability_results_df = pd.DataFrame.from_dict(probability_results, orient=\"index\")\n",
    "probability_results_df.to_csv(probability_results_path)\n",
    "print(f\"Probability results saved to {probability_results_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
