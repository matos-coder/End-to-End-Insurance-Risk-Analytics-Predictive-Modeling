{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Predictive Modeling Notebook\n",
    "\n",
    "# **1. Import Necessary Libraries and Functions**\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src/utils'))\n",
    "from modeling import (\n",
    "    handle_missing_data,\n",
    "    encode_categorical_variables,\n",
    "    split_data,\n",
    "    train_regression_models,\n",
    "    evaluate_regression_models,\n",
    "    train_classification_models,\n",
    "    evaluate_classification_models,\n",
    "    analyze_feature_importance\n",
    ")\n",
    "\n",
    "# **2. Load and Preprocess the Dataset**\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "data_path = \"../data/processed/claim_metrics.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print shape after each major step for debugging\n",
    "def print_shape(label, df):\n",
    "    print(f\"{label} shape: {df.shape}\")\n",
    "\n",
    "print_shape(\"After loading\", df)\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "df = handle_missing_data(df, method=\"impute\")\n",
    "print_shape(\"After first impute\", df)\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "df = encode_categorical_variables(df)\n",
    "print_shape(\"After encoding\", df)\n",
    "\n",
    "# Impute again after encoding to ensure no NaNs remain\n",
    "print(\"Handling missing values after encoding...\")\n",
    "df = handle_missing_data(df, method=\"impute\")\n",
    "print_shape(\"After second impute\", df)\n",
    "\n",
    "# **3. Data Preparation for Claim Severity Prediction**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Claim Severity Prediction...\")\n",
    "target_col_severity = \"TotalClaims\"\n",
    "severity_data = df[df[target_col_severity] > 0]  # Use subset where claims > 0\n",
    "print_shape(\"After filtering TotalClaims > 0\", severity_data)\n",
    "X_train_severity, X_test_severity, y_train_severity, y_test_severity = split_data(severity_data, target_col_severity)\n",
    "print_shape(\"X_train_severity\", X_train_severity)\n",
    "print_shape(\"y_train_severity\", y_train_severity)\n",
    "\n",
    "# Check for missing values in features and target before training\n",
    "print(\"Checking for missing values in training features and target...\")\n",
    "print(\"NaNs in X_train_severity:\", X_train_severity.isna().sum().sum())\n",
    "print(\"NaNs in y_train_severity:\", y_train_severity.isna().sum())\n",
    "\n",
    "if X_train_severity.isna().sum().sum() > 0 or y_train_severity.isna().sum() > 0:\n",
    "    print(\"Dropping rows with NaNs in training data...\")\n",
    "    nan_mask = ~(X_train_severity.isna().any(axis=1) | y_train_severity.isna())\n",
    "    X_train_severity = X_train_severity[nan_mask]\n",
    "    y_train_severity = y_train_severity[nan_mask]\n",
    "    print(\"After dropping, NaNs in X_train_severity:\", X_train_severity.isna().sum().sum())\n",
    "    print(\"After dropping, NaNs in y_train_severity:\", y_train_severity.isna().sum())\n",
    "\n",
    "# Train models for claim severity prediction\n",
    "print(\"Training regression models...\")\n",
    "regression_models = train_regression_models(X_train_severity, y_train_severity)\n",
    "\n",
    "# Evaluate regression models\n",
    "print(\"Evaluating regression models...\")\n",
    "severity_results = evaluate_regression_models(regression_models, X_test_severity, y_test_severity)\n",
    "print(\"Severity Prediction Results:\")\n",
    "print(severity_results)\n",
    "\n",
    "# **4. Data Preparation for Premium Optimization**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Premium Optimization...\")\n",
    "target_col_premium = \"CalculatedPremiumPerTerm\"\n",
    "X_train_premium, X_test_premium, y_train_premium, y_test_premium = split_data(df, target_col_premium)\n",
    "\n",
    "# Train models for premium prediction\n",
    "print(\"Training regression models...\")\n",
    "premium_models = train_regression_models(X_train_premium, y_train_premium)\n",
    "\n",
    "# Evaluate premium prediction models\n",
    "print(\"Evaluating regression models...\")\n",
    "premium_results = evaluate_regression_models(premium_models, X_test_premium, y_test_premium)\n",
    "print(\"Premium Prediction Results:\")\n",
    "print(premium_results)\n",
    "\n",
    "# **5. Claim Probability Prediction (Advanced Task)**\n",
    "# Define target variable and split data\n",
    "print(\"Preparing data for Claim Probability Prediction...\")\n",
    "target_col_probability = \"ClaimOccurred\"  # Binary classification (1 if claim occurred, 0 otherwise)\n",
    "X_train_prob, X_test_prob, y_train_prob, y_test_prob = split_data(df, target_col_probability)\n",
    "\n",
    "# Train classification models\n",
    "print(\"Training classification models...\")\n",
    "classification_models = train_classification_models(X_train_prob, y_train_prob)\n",
    "\n",
    "# Evaluate classification models\n",
    "print(\"Evaluating classification models...\")\n",
    "probability_results = evaluate_classification_models(classification_models, X_test_prob, y_test_prob)\n",
    "print(\"Claim Probability Prediction Results:\")\n",
    "print(probability_results)\n",
    "\n",
    "# **6. Feature Importance Analysis**\n",
    "print(\"Analyzing feature importance...\")\n",
    "for model_name, model in regression_models.items():\n",
    "    print(f\"Analyzing feature importance for {model_name}...\")\n",
    "    analyze_feature_importance(model, X_train_severity)\n",
    "\n",
    "# **7. Save Results**\n",
    "print(\"Saving results...\")\n",
    "severity_results_path = \"../results/severity_results.csv\"\n",
    "severity_results_df = pd.DataFrame.from_dict(severity_results, orient=\"index\")\n",
    "severity_results_df.to_csv(severity_results_path)\n",
    "print(f\"Severity results saved to {severity_results_path}.\")\n",
    "\n",
    "premium_results_path = \"../results/premium_results.csv\"\n",
    "premium_results_df = pd.DataFrame.from_dict(premium_results, orient=\"index\")\n",
    "premium_results_df.to_csv(premium_results_path)\n",
    "print(f\"Premium results saved to {premium_results_path}.\")\n",
    "\n",
    "probability_results_path = \"../results/probability_results.csv\"\n",
    "probability_results_df = pd.DataFrame.from_dict(probability_results, orient=\"index\")\n",
    "probability_results_df.to_csv(probability_results_path)\n",
    "print(f\"Probability results saved to {probability_results_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
